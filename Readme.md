

````markdown
# ğŸ“„ Documind AI â€“ Ask Questions on Your Documents

**Documind AI** is an offline, privacy-first intelligent document question-answering system. It allows users to upload PDF documents and query them using natural language. Responses are generated based on document content using local LLMs like **Mistral** via **Ollama**.

---

## ğŸš€ Features

- ğŸ“ Upload PDFs and parse all text, including scanned text (OCR).
- ğŸ¤– Ask questions and get accurate, contextual answers from your own documents.
- ğŸ” 100% offline processing with no cloud or external dependencies.
- ğŸ§  Uses Ollama + Mistral for local inference.
- ğŸ“¦ Lightweight vector database using FAISS and local JSON storage.

---

## ğŸ“‚ Project Structure

```bash
documind/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py
â”‚   â”‚   â”‚   â””â”€â”€ query.py
â”‚   â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”‚   â””â”€â”€ mistral_embedder.py
â”‚   â”‚   â”œâ”€â”€ chunking/
â”‚   â”‚   â”‚   â””â”€â”€ text_splitter.py
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ document_parser.py
â”‚   â”‚   â”‚   â””â”€â”€ clean_text.py
â”‚   â”‚   â”œâ”€â”€ vector_store/
â”‚   â”‚   â”‚   â””â”€â”€ vector_store.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ query_service.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/ (optional: if using React or HTML frontend)
â””â”€â”€ README.md
````

---

## ğŸ› ï¸ Prerequisites

* Python 3.10 or 3.11
* [Ollama](https://ollama.com/) installed and running locally
* [Git](https://git-scm.com/) installed
* Visual C++ Build Tools (for PyMuPDF and FAISS on Windows)

---

## âš™ï¸ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/documind-ai.git
cd documind-ai/backend
```

### 2. Create and Activate Virtual Environment

```bash
python -m venv .venv
.venv\Scripts\activate  # On Windows
```

### 3. Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

If you get errors with `faiss`, run:

```bash
pip install faiss-cpu
```

---

## ğŸ¤– Run Ollama (Mistral)

Start Ollama and pull the model:

```bash
ollama run mistral
```

This will start the local model server at `http://localhost:11434`.

---

## â–¶ï¸ Run the Backend Server

```bash
uvicorn app.main:app --reload
```

Server will run at `http://127.0.0.1:8000`.

---

## ğŸŒ Frontend Usage

Visit `http://localhost:3000` (if using React frontend), or use Postman/cURL:

### Upload a Document:

```http
POST /api/upload
Form-data: file=<your PDF>
```

### Ask a Question:

```http
POST /api/query
JSON body:
{
  "query": "What is the summary of the document?",
  "source": "filename.pdf"
}
```

---

## ğŸ§ª Sample Queries

* "Summarise the whole document"
* "What is the CGPA mentioned?"
* "List key skills and experience"
* "Who is the author of this resume?"

---

## ğŸ’¾ Data Storage

* Embeddings stored in: `app/vector_store/index.faiss`
* Metadata stored in: `app/vector_store/meta.pkl`

---

## ğŸ”’ Privacy

This app runs entirely on your local machine. No data is sent to the cloud or third-party APIs.

---

## ğŸ™‹â€â™‚ï¸ Troubleshooting

* â— **CropBox missing warning?**

  * Ignore, it comes from `pdfplumber`.

* â— **"I don't know" answers?**

  * Check if the chunking missed your content or your question is too generic.

* â— **FAISS errors?**

  * Ensure `backend/app/vector_store` directory exists.





